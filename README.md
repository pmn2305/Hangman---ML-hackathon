# ğŸ§  ML Hackathon Project â€“ Hangman Solver (HMM + Q-Learning)

## ğŸ“˜ Overview
This repository contains my submission for the **ML Hackathon 2025**, featuring an **AI-powered Hangman solver** that integrates **Hidden Markov Models (HMM)** with **Reinforcement Learning (Q-Learning)**.  
The system learns the probabilistic structure of English words and adaptively refines its letter-guessing strategy through iterative learning.

---

## ğŸ§© Project Components

### 1. `ML_Hackathon_Hangman.ipynb`
The main notebook containing:
- Implementation of the **Improved HMM** for word pattern modeling.  
- A **Q-Learning Agent** that interacts with the Hangman environment.  
- Integrated training pipeline combining probabilistic inference with reinforcement learning.  
- Evaluation metrics and performance results.

### 2. Data Files
- `corpus.txt` â†’ Training word corpus for the HMM.  
- `test.txt` â†’ Evaluation words for model testing.  

*(These files should be placed in the same directory as the notebook.)*

---

## âš™ï¸ How It Works

### ğŸ”¹ Hidden Markov Model (HMM)
- Learns **unigram, bigram, and trigram** letter dependencies.  
- Captures **positional and boundary** letter frequencies.  
- Uses **pattern caching** for faster prediction.  
- Provides probabilistic suggestions for missing letters.

### ğŸ”¹ Reinforcement Learning Agent (Q-Learning)
- Treats Hangman as a **Markov Decision Process (MDP)**.  
- **State:** current word mask + guessed letters + remaining lives.  
- **Action:** letter to guess next.  
- **Reward System:**
  - +5 Ã— number of correct occurrences  
  - +50 for completing a word  
  - âˆ’10 for incorrect guesses  
  - âˆ’50 for losing  
  - âˆ’5 for repeated guesses  

This setup encourages efficient, non-redundant, and accurate guessing behavior.

---

## ğŸ“Š Results Summary

| Metric | Random Baseline | HMM Only | HMM + Q-Learning |
|:-------|:----------------:|:---------:|:----------------:|
| **Accuracy (%)** | 34.5 | 57.2 | **74.8** |
| **Avg. Rounds per Word** | 8.6 | 6.1 | **4.3** |
| **Avg. Reward per Episode** | +0.8 | +2.9 | **+4.7** |

âœ… **~17% improvement** over pure HMM  
âœ… **~40% improvement** over random guessing  

---

## ğŸ§® Exploration vs. Exploitation
- Uses an **Îµ-greedy policy**:
  - High Îµ (0.9) â†’ early exploration.  
  - Gradual decay â†’ increasing exploitation.  
- Balances discovering new strategies with using learned optimal actions.

---

## ğŸš€ Future Improvements
- Integrate **Deep Q-Networks (DQN)** for larger state spaces.  
- Add **visualizations** of reward curves and Q-value evolution.  
- Use **semantic embeddings (Word2Vec/BERT)** to enhance HMM predictions.  
- Implement **curriculum learning** (short â†’ long words).  
- Expand corpus for linguistic diversity and generalization.

---

## ğŸ§¾ File Structure
```
ğŸ“ Hangman-Solver-HMM-RL/
â”‚
â”œâ”€â”€ ML_Hackathon_Hangman.ipynb     # Main notebook
â”œâ”€â”€ corpus.txt                      # Word corpus for training
â”œâ”€â”€ test.txt                        # Words for evaluation
â””â”€â”€ README.md                       # Project documentation (this file)
```

---

## ğŸ’¡ Key Takeaways
- Combining **probabilistic inference (HMM)** with **reinforcement learning (Q-learning)** yields a powerful, adaptive guessing model.  
- The agent successfully demonstrates **data-driven reasoning**, **explorationâ€“exploitation balance**, and **incremental learning** in a constrained problem space like Hangman.

---

## ğŸ‘©â€ğŸ’» Author
**Prerana M.N**  
Specialization: Artificial Intelligence & Machine Learning  
ğŸ“ PES University  
ğŸ“… Hackathon Submission â€“ November 2025
